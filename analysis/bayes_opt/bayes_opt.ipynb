{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "from imblearn.ensemble import EasyEnsembleClassifier  # adaboost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-24f37d84160b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_metrics(y_true, y_pred, y_score):# 传入真实值预测值 返回MAE RMSE MAPE R2的列表\n",
    "    ans=[]\n",
    "    ans.append(accuracy_score(y_true,y_pred))\n",
    "    ans.append(precision_score(y_true,y_pred))\n",
    "    ans.append(recall_score(y_true,y_pred))\n",
    "    ans.append(roc_auc_score(y_true,y_score))\n",
    "    ans.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    ans.append(f1_score(y_true, y_pred))\n",
    "    ans.append(confusion_matrix(y_true, y_pred))\n",
    "    return ans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def customize_score(true_value, predict):\n",
    "    return precision_score(true_value, predict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def xgb_cv(max_depth,learning_rate,n_estimators,gamma,subsample,\n",
    "            reg_alpha,reg_lambda):  # 传入参数\n",
    "    params = {}\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['learning_rate'] = learning_rate\n",
    "    params['n_estimators'] = int(n_estimators)\n",
    "    params[\"gamma\"] = gamma\n",
    "    params['subsample'] = subsample\n",
    "    params['reg_alpha'] = reg_alpha\n",
    "    params['reg_lambda'] = reg_lambda\n",
    "\n",
    "    my_scorer = make_scorer(customize_score, greater_is_better=True)\n",
    "    base_learner = xgbsk.XGBClassifier(max_depth=7, learning_rate=0.05, n_estimators=10,  # 第三个数据集250\n",
    "                                           verbosity=0, objective=\"binary:logistic\",\n",
    "                                           n_jobs=-1, gamma=0, min_child_weight=0.5, max_delta_step=0,\n",
    "                                           subsample=1, colsample_bytree=0.5, colsample_bylevel=1,\n",
    "                                           reg_alpha=0, reg_lambda=1, scale_pos_weight=1,  # init weight\n",
    "                                           random_state=666, missing=0, importance_type=\"total_gain\",\n",
    "                                           use_label_encoder=False\n",
    "                                           , eval_metric='logloss')\n",
    "    base_learner.set_params(**params)#设置参数\n",
    "    model = EasyEnsembleClassifier(sampling_strategy=0.5, n_jobs=1, verbose=0, n_estimators=10,\n",
    "                                       base_estimator=base_learner, random_state=0)\n",
    "\n",
    "    # 使用precision来评估\n",
    "    scores = cross_val_score(model, df_train.loc[:, \"volume_vale_1\":].values\n",
    "                                 , df_train.loc[:, \"label\"].values, cv=4, scoring=my_scorer, n_jobs=-1)\n",
    "    return np.mean(scores)  # 返回评价指标  最小化还是最大化"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_test_performance(df_train, best_params):\n",
    "\n",
    "    base_learner = XGBClassifier(max_depth=7, learning_rate=0.05, n_estimators=10,  # 第三个数据集250\n",
    "                                       verbosity=0, objective=\"binary:logistic\",\n",
    "                                       n_jobs=-1, gamma=0, min_child_weight=0.5, max_delta_step=0,\n",
    "                                       subsample=1, colsample_bytree=0.5, colsample_bylevel=1,\n",
    "                                       reg_alpha=0, reg_lambda=1, scale_pos_weight=1,  # init weight\n",
    "                                       random_state=666, missing=0, importance_type=\"total_gain\",\n",
    "                                       use_label_encoder=False\n",
    "                                       , eval_metric='logloss')\n",
    "    base_learner.set_params(**best_params)  # 设置参数\n",
    "    model = EasyEnsembleClassifier(sampling_strategy=0.5, n_jobs=1, verbose=0, n_estimators=10,\n",
    "                                   base_estimator=base_learner, random_state=0)\n",
    "\n",
    "    model.fit(df_train.loc[:, \"volume_vale_1\":].values, df_train.loc[:, \"label\"].values)\n",
    "    print(\"-------\")\n",
    "    ans_label = model.predict(df_train.loc[:, \"volume_vale_1\":].values);  # 2\n",
    "    print(\" 训练集的结果 label=1 的个数\", sum(ans_label))\n",
    "    ans_score = model.predict_proba(df_train.loc[:, \"volume_vale_1\":].values);  # 2\n",
    "    test_ans = get_metrics(df_train.loc[:, \"label\"].values, ans_label, ans_score[:, 1])\n",
    "    print(\"训练集metrics  acc=%.3f precision=%.3f recall=%.3f \\n auc=%.3f balanced_acc=%.3f f1=%.3f\" % (test_ans[0],\n",
    "             "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def split_data(df_all, method=\"order\"):#划分数据集\n",
    "    with open(data_path,\"rb\") as f:\n",
    "        df_train=pickle.load(f)\n",
    "    df_train[\"label\"]=df_train[\"ratio\"].apply(lambda x:1 if x>1.5 else 0)\n",
    "    df_train=df_train.sort_values(\"date_start\",ascending=True).reset_index(drop=True)\n",
    "\n",
    "    train_index=[i for i in range(df_train.shape[0]) if i<=int(df_train.shape[0]*0.8)]\n",
    "    test_index = [i for i in range(df_train.shape[0]) if i > int(df_train.shape[0] * 0.8)]\n",
    "    df_test=df_train.iloc[test_index,:].reset_index(drop=True)\n",
    "    df_train=df_train.iloc[train_index,:].reset_index(drop=True)\n",
    "    return df_train,df_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def go(df_train, df_test):\n",
    "    past = time.time()\n",
    "    print(\"生成一个网络数据集之后 用模型对他进行评价\")\n",
    "    print(\"训练集大小\",df_train.shape)\n",
    "    print(\"测试集大小\",df_test.shape)\n",
    "\n",
    "\n",
    "    xgb_bo = BayesianOptimization(xgb_cv,\n",
    "        {'max_depth': (4, 20),\n",
    "         'learning_rate': (0.001, 0.5),\n",
    "         'n_estimators': (10, 500),\n",
    "         'gamma': (0.001, 3),\n",
    "         'subsample': (0.2, 0.99),\n",
    "         'reg_alpha': (0.01, 0.99),\n",
    "         'reg_lambda': (0.01, 0.99)})\n",
    "    xgb_bo.maximize(init_points=5, n_iter=2000)  # init_points表示初始点，n_iter代表迭代次数（即采样数）\n",
    "    print(\"调参结果\",xgb_bo.max)\n",
    "    with open(\"best_params.pkl\", \"wb\") as f:\n",
    "        pickle.dump(xgb_bo.max, f)#保存\n",
    "\n",
    "    get_test_performance(df_train,df_test,xgb_bo.max)\n",
    "    print(\"消耗时间\", (time.time() - past) / 60)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train,df_test=split_data(data_path,method=\"order\")#[:,\"volume_differ_1\":\"refund_10-15差值\"]\n",
    "print(df_train.shape,df_test.shape)\n",
    "for model in [\"Boosting\"]:\n",
    "    print(\"----当前任务----\", model)\n",
    "    go(df_train,df_test)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('py38': conda)"
  },
  "interpreter": {
   "hash": "91aaf874a35f6947e93d056613dff7c1e5077b04cc56616f8d938977ea56f218"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}